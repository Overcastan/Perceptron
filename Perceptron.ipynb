{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron. Stochastic gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "from mnist import MNIST\n",
    "import random\n",
    "\n",
    "mndata = MNIST('C:\\\\Users\\\\ivan_\\\\PycharmProjects\\\\optimization\\\\samples')\n",
    "\n",
    "images, labels = mndata.load_testing()\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Класс перцептрона\n",
    "Вход            -   столбец $ W_{in} \\;[ 784 \\times 1] $, $ 784 = 28 \\times 28 $ -- размер исходного изображения  \n",
    "Первый слой     -   матрица $ W_{1} \\; [16 \\times 784] $  \n",
    "Второй слой     -   матрица $ W_{2} \\; [10 \\times 16] $  \n",
    "Выходной слой   -   столбец $ W_{out} \\; [10 \\times 1] $  \n",
    "В выходном слое в элементе $W_{out}[i], i \\in [0,9]$ содержится оценка вероятности того, что на вход подано изображение числа $i$  \n",
    "В качестве функции активации используется сигмоида $\\sigma (x) = \\frac{1}{1 + e^{-x}}$  \n",
    "Для нормализации выходных значений используется softmax-преобразование: $p[i] = \\frac{e^{y[i]}}{T}, T = \\sum\\limits_{k = 0}^{9}{e^{y[k]}}, i \\in [0,9]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    w_in = []   # vector with input data -- pixels of input picture represented as a vector\n",
    "    w_out = []  # result -- vector with probabilities of events. Event i is numbers i in [0-9] is on the picture\n",
    "    ans = -1    # correct answer in [0-9]\n",
    "    \n",
    "    layers = [] # [w_1, b_2, w_2, b_2]\n",
    "    w_1 = [][]  # matrix with coeffs of 1st hidden layer \n",
    "    b_1 = [][]  \n",
    "    w_2 = [][]  # matrix with coeffs of 2nd hidden layer\n",
    "    b_2 = [][]\n",
    "    \n",
    "    w_in_dim = 0  # size of input vector\n",
    "    w_1_dim = 0   # number of rows in 1st hidden layer\n",
    "    w_out_dim = 0 # size of output vector\n",
    "    \n",
    "    # Matrix sizes\n",
    "    \n",
    "    # w_in:  [w_in_dim  x 1        ]\n",
    "    # w_1:   [w_1_dim   x w_in_dim ]\n",
    "    # b_1:   [w_1_dim   x 1        ]\n",
    "    # w_2:   [w_out_dim x w_1_dim  ]\n",
    "    # b_2:   [w_out_dim x 1        ]\n",
    "    # w_out: [w_out_dim x 1        ]\n",
    "    \n",
    "    derivatives = [] # [w_1_derivative, b_2_derivative, w_2_derivative, b_2_derivative]\n",
    "    w_1_derivative = [][] # matrix with partial derivatives of parameters in 1st hidden layer\n",
    "    b_1_derivative = [][]\n",
    "    w_2_derivative = [][] # matrix with partial derivatives of parameters in 2nd hidden layer\n",
    "    b_2_derivative = [][]\n",
    "    \n",
    "    test_num = 0  # 0 in the beginning of learning. Increases when picture is processed\n",
    "    epoch_num = 0 # 0 in the beginning of learning. Increases when whole battery of test pictures is processed\n",
    "    \n",
    "    error = [] # array of errors of all the tests\n",
    "    \n",
    "    act_func = lambda x: 1 / (1 + np.exp(-x)) # current activation function\n",
    "    \n",
    "\n",
    "    def __init__(self, w_in_dim, w_1_dim, w_out_dim):\n",
    "        self.w_in_dim = w_in_dim\n",
    "        self.w_1_dim = w_1_dim\n",
    "        self.w_out_dim = w_out_dim\n",
    "        \n",
    "        self.w_in = np.ones(w_in_dim)\n",
    "        self.w_out = np.zeros(w_out_dim)\n",
    "        \n",
    "        self.w_1 = np.ones((w_1_dim, w_in_dim))\n",
    "        self.b_1 = np.ones(w_1_dim)\n",
    "        self.w_2 = np.ones((w_out_dim, w_1_dim))\n",
    "        self.b_2 = np.ones(w_out_dim)\n",
    "        self.layers = [self.w_1, self.b_2, self.w_2, self.b_2]\n",
    "        \n",
    "        self.w_1_derivative = np.ones((w_1_dim, w_in_dim))\n",
    "        self.b_1_derivative = np.ones(w_1_dim)\n",
    "        self.w_2_derivative = np.ones((w_out_dim, w_1_dim))\n",
    "        self.b_2_derivative = np.ones(w_out_dim)\n",
    "        self.derivatives = [self.w_1_derivative, self.b_2_derivative, self.w_2_derivative, self.b_2_derivative]\n",
    "        \n",
    "    def get_activation_func(self):\n",
    "        return self.act_func\n",
    "\n",
    "    \n",
    "    def set_activation_func(self, func):\n",
    "        self.act_func = func\n",
    "    \n",
    "    \n",
    "    def softmax(self):\n",
    "        self.w_out = np.exp(self.w_out) / np.sum(np.exp(self.w_out))\n",
    "    \n",
    "    \n",
    "    def calc_result(self):\n",
    "        self.w_out = self.w_2 @ self.activation_func(self.w_1 @ self.w_in)\n",
    "        self.softmax()\n",
    "    \n",
    "    \n",
    "    def teach_with_picture(self, w_in, ans):\n",
    "        self.w_in = w_in\n",
    "        self.ans = ans\n",
    "        \n",
    "        # auxilary vectors\n",
    "        M = w_1 @ w_in + b_1\n",
    "        N = w_2 @ self.act_func(M).T + b_2\n",
    "        N_stroke = w_2 * np.tile((np.ones(w_1_dim) + np.exp(-M)) ** -2 * np.exp(-M), w_out_dim).reshape(w_out_dim, w_1_dim)\n",
    "        \n",
    "        exp_sum = np.sum(N)\n",
    "        indicators = np.zeros(w_out_dim)\n",
    "        indicators[self.ans] = 1\n",
    "                \n",
    "        b_2_derivative = (N - indicators) * np.exp(N) * (np.ones(w_out_dim) / exp_sum - np.exp(N) / exp_sum ** 2)\n",
    "        w_2_derivative = b_2_derivative.reshape(w_out_dim, 1) * self.act_func(M).reshape(1, w_1_dim)\n",
    "        \n",
    "        b_1_derivative = np.zeros(w_1_dim)\n",
    "        for k in range(w_out_dim):\n",
    "            b_1_derivative += ((w_out[k] - indicators[k]) * np.exp(N[k]) * np.exp(N).reshape(1, w_out_dim) @ (\n",
    "                np.tile(N_stroke[k], w_out_dim).reshape(w_out_dim, w_1_dim) - N_stroke))[0]\n",
    "        w_1_derivative = b_1_derivative.reshape(w_1_dim, 1) @ w_in.reshape(1, w_in_dim)\n",
    "\n",
    "\n",
    "    def get_derivative(self, matrix_num, i, j):\n",
    "        return self.layers[matrix_num][i][j]\n",
    "    \n",
    "    \n",
    "    def update_weights(self):\n",
    "        for matrix_num in range(len(self.layers):\n",
    "            for i in range(len(self.layers[k]):\n",
    "                for j in range(len(self.layers[k][0]):\n",
    "                    self.layers[k][i][j] += self.eta * get_derivative(matrix_num, i, j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
